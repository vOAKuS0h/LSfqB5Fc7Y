{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Tweets\n",
    "This notebooks is used for parsing tweets from the twitter API and extracting the images for those tweets that contain them using pyspark. Then using the s3 api we post the images to S3 bucket for further processing. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "* PySpark\n",
    "* Anaconda Python\n",
    "* AWS Boto Python module\n",
    "* DIGITS Webserver running with a model for classification\n",
    "* Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "import zipfile\n",
    "import pycurl\n",
    "from io import BytesIO\n",
    "import pycurl\n",
    "from io import BytesIO\n",
    "import boto\n",
    "import numpy as np\n",
    "import pycurl\n",
    "import shutil\n",
    "import urllib\n",
    "from StringIO import StringIO\n",
    "import json\n",
    "import boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameters that need to be set \n",
    "\n",
    "aws_access_key_id = 'my_access_key_id'\n",
    "aws_secret_access_key = 'mysecret_access_key'\n",
    "deep_learning_digits_server_ip = 'digits_ip'\n",
    "\n",
    "# We start with some tweets that exist on HDFS. You can\n",
    "# collect your own tweets using the twitter API and \n",
    "# use them as a starting point.\n",
    "\n",
    "# Read in json text files of tweets\n",
    "tweetsRdd = sc.textFile(\"combined/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "#print the json in a pretty way\n",
    "def pretty(d, indent=0):\n",
    "   for key, value in d.iteritems():\n",
    "      print '\\t' * indent + str(key)\n",
    "      if isinstance(value, dict):\n",
    "         pretty(value, indent+1)\n",
    "      #else:\n",
    "      #   print '\\t' * (indent+1) + str(value)\n",
    "def getMediaUrl(obj):\n",
    "    items = []\n",
    "    for item in getNestedKeyValue(obj, [\"media\"]):\n",
    "        items.append([obj['id'],getNestedKeyValue(item, [\"media_url\"])])\n",
    "    return items\n",
    "\n",
    "#find an item in the dictionary\n",
    "def _finditem(obj, key):\n",
    "    if key in obj: return obj[key]\n",
    "    for k, v in obj.items():\n",
    "        if isinstance(v,dict):\n",
    "            item = _finditem(v, key)\n",
    "            if item is not None:\n",
    "                return item    \n",
    "\n",
    "#return an item the \n",
    "def getNestedKeyValue(obj, keys):\n",
    "    for num in range(len(keys)):\n",
    "        if isinstance(obj, type({})):\n",
    "            obj = _finditem(obj, keys[num])\n",
    "        if obj is None:\n",
    "            return \"\"\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCoordinates(obj):\n",
    "    x = getNestedKeyValue(obj, [\"location\", \"geo\", \"coordinates\"])\n",
    "    if x == \"\" or x is None:\n",
    "        return \"\"\n",
    "\n",
    "    x = x[0]\n",
    "    return [[(x[0][0] + x[2][0])/2, (x[0][1] + x[1][1])/2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def posturlS3(tweetid_imgurlpair, aws_access_key_id=None, aws_secret_access_key=None):\n",
    "    bucketid = 'none'\n",
    "    tweetid = tweetid_imgurlpair[0]\n",
    "    imgurl = tweetid_imgurlpair[1]\n",
    "    \n",
    "    ext = \".\"+ imgurl.split('.')[-1]\n",
    "    iduniq = tweetid.split(\":\")[-1]\n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    c = pycurl.Curl()\n",
    "    c.setopt(c.URL,  imgurl)\n",
    "    c.setopt(c.WRITEDATA, buffer)\n",
    "    c.perform()\n",
    "    #c.close()\n",
    "    \n",
    "    s3c = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\n",
    "    bucket = s3c.lookup(bucketid)\n",
    "    key = bucket.key_class(b,iduniq+ext)\n",
    "    key.set_contents_from_string(buffer.getvalue())\n",
    "    return [c.getinfo(c.RESPONSE_CODE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to query the GPU server for the classification category parse the JSON and add the tweet ID\n",
    "def getClassificationJson(tweetid, job_id='20150706-203919-1597', serverip=deep_learning_digits_server_ip):\n",
    "    c = pycurl.Curl()\n",
    "    imageurl = \"",
    "    digitsurl = \"http://%s/models/images/classification/classify_one.json\"% (serverip)\n",
    "    buf = StringIO()\n",
    "    c.setopt(c.URL, digitsurl)\n",
    "    c.setopt(c.POSTFIELDS, \\\n",
    "             urllib.urlencode((('job_id',job_id),('image_url',imageurl))))\n",
    "    c.setopt(c.WRITEDATA, buf)\n",
    "    c.setopt(c.POST, 1)\n",
    "    c.perform()\n",
    "    print \"%s?%s\" % (digitsurl,urllib.urlencode((('job_id',job_id),('image_url',imageurl))))\n",
    "    print c.getinfo(c.RESPONSE_CODE)\n",
    "    if c.getinfo(c.RESPONSE_CODE) == 200:\n",
    "        jdict = json.loads(buf.getvalue())\n",
    "        jdict['tweet_id'] = tweetid\n",
    "        return [json.dumps(jdict)]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to query the GPU server for the classification category parse the JSON and add the tweet ID\n",
    "def getHardhatClassificationJson(tweetid):\n",
    "    hardhatmodel_job_id = '20151030-223956-5b04'\n",
    "    return getClassificationJson(tweetid, job_id=hardhatmodel_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#map the parse the json objects in the tweets\n",
    "jsondict = tweetsRdd.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#map the tweet IDs\n",
    "tweetIDMapRDD = jsondict.map(lambda x:  x['id'].split(\":\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the list of image urls from the twitter posts\n",
    "mediaRdd = jsondict.flatMap(lambda x: getMediaUrl(x))\n",
    "\n",
    "# post the images to an S3 bucket for ease of access\n",
    "responses = mediaRdd.flatMap(lambda x: posturlS3(x) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "responsearray = np.array(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print diagnostics about the http responses for gathering the twitter images\n",
    "\n",
    "print \"Number of correct responses: \", sum(responsearray==200), \"\\n\", \"Number of redirects (307):\",\\\n",
    "    sum(responsearray==307),\"\\nNumber of forbidden (403): \", sum(responsearray==403),\\\n",
    "    \"\\nNumber of page not found (404): \", sum(responsearray==404),\\\n",
    "    \"\\nNumber of service unavailable (503): \", sum(responsearray==503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweetIDMapRDD = jsondict.map(lambda x:  x['id'].split(\":\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifyJsonHardhatRDD = tweetIDMapRDD.flatMap(lambda x: getHardhatClassificationJson(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifyJsonRDD = tweetIDMapRDD.flatMap(lambda x: getClassificationJson(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifyJsonRDD.saveAsTextFile(\"tweetPredictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifyJsonHardhatRDD.saveAsTextFile(\"tweetPredictionsHardHat.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
